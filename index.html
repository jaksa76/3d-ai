<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>3D AI Project</title>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/three.js/r128/three.min.js"></script>    
    <style>
        body {
            margin: 0;
            overflow: hidden;
            display: flex;
            justify-content: center;
            align-items: center;
            height: 100vh;
            width: 100vw;
            display: flex;
            align-items: center;
        }

        #app {
            position: relative;
            background-color: antiquewhite;
            display: flex;
            align-items: center;
        }

        #canvas-container {
            width: 800px;
            height: 600px;
            margin: auto;
        }

        #record-button {
            position: absolute;
            bottom: 20px;
            right: 20px;
            width: 50px;
            height: 50px;
            background-color: rgb(255, 255, 255);
            border-radius: 50%;
            text-align: center;
            line-height: 48px;
            font-size: 24px;
            border: 1 solid red;
        }
    </style>
</head>
<body>
    <div id="app">
        <div id="canvas-container"></div>
        <button id="record-button">ðŸŽ¤</button>
    </div>
    <script lang="javascript">
        // Generate a unique session ID
        const sessionId = '_' + Math.random().toString(36).substr(2, 9);

        // Voice recognition setup
        const recordButton = document.getElementById('record-button');
        const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
        const recognition = new SpeechRecognition();
        recognition.lang = 'en-US';
        recognition.interimResults = false;

        recognition.onresult = function(event) {
            recordButton.style.backgroundColor = 'rgb(255, 255, 255)';
            const voiceCommand = event.results[0][0].transcript;
            console.log('Voice Command:', voiceCommand);

            // Capture the canvas image
            const canvas = document.querySelector('canvas');
            const imageData = canvas.toDataURL('image/png');

            // Send voice command and image data to backend
            fetch('/voice-command', {
                method: 'POST',
                headers: {
                    'Content-Type': 'application/json'
                },
                body: JSON.stringify({ sessionId, command: voiceCommand, imageData })
            })
            .then(response => response.json())
            .then(data => {
                console.log('Success:', data);
                console.log('Scene Update:', data.sceneUpdate);
                eval(data.sceneUpdate);
            }).catch((error) => console.error('Error:', error));
        };

        recognition.onerror = function(event) {
            recordButton.style.backgroundColor = 'rgb(255, 255, 255)';
            console.error('Speech recognition error', event.error);
        };

        recordButton.addEventListener('click', () => {
            recordButton.style.backgroundColor = 'rgb(255, 0, 0)';
            recognition.start();
        });


        // setup 3D scene
        const canvasContainer = document.getElementById('canvas-container');
        const scene = new THREE.Scene();
        const camera = new THREE.PerspectiveCamera(75, window.innerWidth / window.innerHeight, 0.1, 1000);
        const renderer = new THREE.WebGLRenderer();
        renderer.setSize(800, 600);
        canvasContainer.appendChild(renderer.domElement);
        camera.position.z = 5;

        function animate() {
            requestAnimationFrame(animate);
            renderer.render(scene, camera);
        }
        animate();

        window.objects = {};
    </script>
</body>
</html>
